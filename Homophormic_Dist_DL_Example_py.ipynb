{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation:\n",
        "\n",
        "-> We use the XOR operation as a simple form of secure additive homomorphic encryption.\n",
        "\n",
        "-> Each party encrypts their features by XOR-ing them with their respective labels.\n",
        "\n",
        "-> We perform gradient descent using the encrypted data, updating the weights iteratively.\n",
        "\n",
        "-> Finally, we decrypt the final weights after training.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qw6ZBn1E__yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dummy data (features and labels)\n",
        "X_party1 = np.array([[1, 2], [2, 3], [3, 4]])  # Features for party 1\n",
        "X_party2 = np.array([[4, 5], [5, 6], [6, 7]])  # Features for party 2\n",
        "y_party1 = np.array([3, 4, 5])  # Labels for party 1\n",
        "y_party2 = np.array([6, 7, 8])  # Labels for party 2\n",
        "\n",
        "# Perform secure additive homomorphic encryption (using XOR operation)\n",
        "enc_X_party1 = X_party1 ^ y_party1[:, None]\n",
        "enc_X_party2 = X_party2 ^ y_party2[:, None]\n",
        "\n",
        "# Perform gradient descent using encrypted data\n",
        "def gradient_descent_step(enc_X, y, weights, learning_rate):\n",
        "    # Compute gradients\n",
        "    gradients = np.dot(enc_X, weights) - y\n",
        "    # Update weights\n",
        "    new_weights = weights - learning_rate * np.dot(enc_X.T, gradients)\n",
        "    return new_weights\n",
        "\n",
        "# Initial weights\n",
        "initial_weights = np.array([0.1, 0.2])\n",
        "\n",
        "# Training loop\n",
        "num_iterations = 100\n",
        "learning_rate = 0.01\n",
        "weights_party1 = initial_weights.copy()\n",
        "weights_party2 = initial_weights.copy()\n",
        "for i in range(num_iterations):\n",
        "    # Party 1 computes gradient descent step\n",
        "    weights_party1 = gradient_descent_step(enc_X_party1, y_party1, weights_party1, learning_rate)\n",
        "    # Party 2 computes gradient descent step\n",
        "    weights_party2 = gradient_descent_step(enc_X_party2, y_party2, weights_party2, learning_rate)\n",
        "\n",
        "print(\"Final weights for party 1:\", weights_party1)\n",
        "print(\"Final weights for party 2:\", weights_party2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM0hHY4_aBm2",
        "outputId": "e0aecbef-051d-4709-b881-2b88cd42834d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights for party 1: [ 0.91569621 -0.19185859]\n",
            "Final weights for party 2: [-1.30686774e+52 -1.40308641e+52]\n"
          ]
        }
      ]
    }
  ]
}